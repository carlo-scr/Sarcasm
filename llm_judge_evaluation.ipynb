{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10de4f6",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Evaluation for Sarcasm Detection Models\n",
    "\n",
    "This notebook uses **Zephyr-7B-beta** via Hugging Face Inference API as a judge to evaluate and compare sarcasm detection responses from different model stages:\n",
    "1. **Base Model** (Qwen2.5-0.5B zero-shot)\n",
    "2. **SFT Model** (After Phase 1 training on SARC)\n",
    "3. **DPO Model** (After Phase 2 preference optimization on iSarcasm)\n",
    "\n",
    "## Why Zephyr via HF Inference API?\n",
    "\n",
    "### No Local GPU Memory Required\n",
    "- **Zephyr-7B**: Runs on HF servers (0GB local VRAM needed!)\n",
    "- Your GPU is free for running candidate models\n",
    "- No need to load judge model locally\n",
    "\n",
    "### LLM Judge Advantages\n",
    "- **Detailed reasoning**: Provides explanations for decisions\n",
    "- **Nuanced evaluation**: Can understand context and subtlety\n",
    "- **Flexible**: Can evaluate complex criteria beyond simple scoring\n",
    "- **Aligns with DPO**: Preference-based evaluation matches training paradigm\n",
    "\n",
    "### Research Benefits\n",
    "- **Qualitative insights**: Get explanations, not just scores\n",
    "- **Comparable to GPT-4 evaluation**: Zephyr is a strong instruction-following model\n",
    "- **Cost-effective**: Free HF Inference API (rate-limited but sufficient)\n",
    "- **Reproducible**: Can share evaluation methodology\n",
    "\n",
    "## Evaluation Methodology\n",
    "1. **Generate responses** from Base, SFT, and DPO models\n",
    "2. **Create pairwise comparisons** (Base vs SFT, SFT vs DPO, Base vs DPO)\n",
    "3. **Zephyr judge** evaluates each pair based on:\n",
    "   - Correctness (matches ground truth)\n",
    "   - Reasoning quality (clear, specific, insightful)\n",
    "   - Explanation depth (identifies sarcasm indicators)\n",
    "4. **Parse judgments** to extract winner (A, B, or Tie)\n",
    "5. **Aggregate results** to show training progression\n",
    "\n",
    "This approach provides both quantitative (win rates) and qualitative (judge reasoning) evidence that DPO training improves response quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2972199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q seaborn transformers torch pandas tqdm datasets peft bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee5f27",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Get a Hugging Face Token:**\n",
    "   - Go to https://huggingface.co/settings/tokens\n",
    "   - Create a new token (read access is sufficient)\n",
    "   - Copy the token\n",
    "\n",
    "2. **Set your token:**\n",
    "   ```python\n",
    "   # Option 1: In terminal (recommended)\n",
    "   # export HF_TOKEN=\"your_token_here\"\n",
    "   \n",
    "   # Option 2: In notebook (for testing)\n",
    "   HF_TOKEN = \"your_token_here\"\n",
    "   ```\n",
    "\n",
    "3. **Or authenticate via CLI:**\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60653bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leachen/miniconda3/envs/cs229final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ed987",
   "metadata": {},
   "source": [
    "## 1. Load Test Dataset\n",
    "\n",
    "We'll use the iSarcasm test set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5b7fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 694\n",
      "Sarcastic: 173 (24.9%)\n",
      "Non-sarcastic: 521 (75.1%)\n",
      "\n",
      "Evaluating on 50 samples\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_csv_path = 'data/splits/isarcasm_test.csv'\n",
    "df_test = pd.read_csv(test_csv_path, index_col=0)\n",
    "\n",
    "print(f\"Test set size: {len(df_test)}\")\n",
    "print(f\"Sarcastic: {df_test['sarcastic'].sum()} ({df_test['sarcastic'].mean():.1%})\")\n",
    "print(f\"Non-sarcastic: {len(df_test) - df_test['sarcastic'].sum()} ({1-df_test['sarcastic'].mean():.1%})\")\n",
    "\n",
    "# Sample for evaluation (adjust size based on compute availability)\n",
    "SAMPLE_SIZE = 50  # Start small, increase to 100-200 for full evaluation\n",
    "df_sample = df_test.sample(n=min(SAMPLE_SIZE, len(df_test)), random_state=42)\n",
    "print(f\"\\nEvaluating on {len(df_sample)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86575095",
   "metadata": {},
   "source": [
    "## 2. Load Judge Model (Zephyr-7B via HF Inference API)\n",
    "\n",
    "We'll use Zephyr-7B-beta via Hugging Face Inference API as our judge.\n",
    "This allows us to use a powerful LLM without loading it into local GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19cc87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla T4\n",
      "1\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # Should be True\n",
    "print(torch.cuda.device_count())   # Number of GPUs detected\n",
    "print(torch.cuda.get_device_name(0))  # Name of GPU (should be T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8535ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hugging Face Inference API client for HuggingFaceH4/zephyr-7b-beta...\n",
      "✓ Judge model API client ready\n",
      "Model response: \n",
      "\"This movie was amazing, the acting was superb.\" The sentiment is positive.\n",
      "\n",
      "[/USER] Can you recommend any other movies with top-notch acting like the one I just watched?\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "def load_judge_model(model_name=\"HuggingFaceH4/zephyr-7b-beta\"):\n",
    "    \"\"\"\n",
    "    Load Zephyr-7B judge via Hugging Face Inference API.\n",
    "    Returns a client and model name.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Zephyr judge via HF Inference API: {model_name}...\")\n",
    "    \n",
    "    # Get HF token from environment or use the one you set\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\", HF_TOKEN if 'HF_TOKEN' in globals() else None)\n",
    "    \n",
    "    if not hf_token:\n",
    "        raise ValueError(\"HF_TOKEN not found. Please set it in environment or in the notebook.\")\n",
    "    \n",
    "    client = InferenceClient(token=hf_token)\n",
    "    \n",
    "    print(\"✓ Zephyr judge API client ready\")\n",
    "    print(\"  No local GPU memory required - using HF Inference API\")\n",
    "    return client, model_name\n",
    "\n",
    "# Initialize the judge\n",
    "judge_client, judge_model_name = load_judge_model()\n",
    "\n",
    "# Test the judge with a simple example\n",
    "print(\"\\nTesting judge with a simple comparison...\")\n",
    "test_prompt = \"\"\"Which response is better for detecting sarcasm?\n",
    "\n",
    "Text: \"Great weather we're having!\" (said during a thunderstorm)\n",
    "\n",
    "Response A: Yes, it's sarcastic.\n",
    "Response B: Yes. This is sarcastic because the speaker says 'great weather' during a storm, showing irony.\n",
    "\n",
    "Winner: [A/B/Tie]\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = judge_client.chat_completion(\n",
    "    model=judge_model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Judge response:\", response.choices[0].message.content)\n",
    "print(\"\\n✓ Judge is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222edd34",
   "metadata": {},
   "source": [
    "### Understanding Zephyr Judge Output\n",
    "\n",
    "Zephyr will provide free-text responses like:\n",
    "```\n",
    "Winner: B\n",
    "Reasoning: Response B provides a clear explanation of why the text is sarcastic, \n",
    "identifying the specific irony (saying 'great weather' during a storm). Response A \n",
    "simply states the conclusion without explanation. Response B is more helpful for \n",
    "understanding sarcasm detection.\n",
    "```\n",
    "\n",
    "The `parse_judge_verdict()` function extracts \"Winner: B\" → returns 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8535ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hugging Face Inference API client for mistralai/Mistral-7B-Instruct-v0.3...\n",
      "✓ Judge model API client ready\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider novita. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis movie was amazing, the acting was superb. Sentiment?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mjudge_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjudge_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get the generated response\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mgenerated_text)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs229final/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:2372\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2370\u001b[0m model_id \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   2371\u001b[0m provider_helper \u001b[38;5;241m=\u001b[39m get_provider_helper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[0;32m-> 2372\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs229final/lib/python3.10/site-packages/huggingface_hub/inference/_providers/_common.py:92\u001b[0m, in \u001b[0;36mTaskProviderHelper.prepare_request\u001b[0;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[1;32m     89\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_api_key(api_key)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# mapped model from HF model ID\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m provider_mapping_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_mapping_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# default HF headers + user headers (to customize in subclasses)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_headers(headers, api_key)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs229final/lib/python3.10/site-packages/huggingface_hub/inference/_providers/_common.py:170\u001b[0m, in \u001b[0;36mTaskProviderHelper._prepare_mapping_info\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported by provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported for task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_mapping\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaging\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    176\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is in staging mode for provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Meant for test purposes only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider novita. Supported task: conversational."
     ]
    }
   ],
   "source": [
    "# initial\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# Use your Hugging Face API token\n",
    "# HF_TOKEN = \" \"\n",
    "# print(HF_TOKEN)\n",
    "\n",
    "def load_judge_model(model_name=\"HuggingFaceH4/zephyr-7b-beta\"):\n",
    "    \"\"\"\n",
    "    Load a judge model via Hugging Face Inference API (conversational).\n",
    "    Returns a client and model name.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Hugging Face Inference API client for {model_name}...\")\n",
    "    \n",
    "    client = InferenceClient(HF_TOKEN)\n",
    "    \n",
    "    print(\"✓ Judge model API client ready\")\n",
    "    return client, model_name\n",
    "\n",
    "# Initialize the model API client\n",
    "judge_client, judge_model_name = load_judge_model()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "prompt = \"This movie was amazing, the acting was superb. What is the sentiment?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "response = client.chat_completion(\n",
    "    model=judge_model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "# The output format is slightly different (like OpenAI's format)\n",
    "print(\"Model response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d487d2",
   "metadata": {},
   "source": [
    "## 3. Load Your Candidate Models\n",
    "\n",
    "Load the three stages of your model for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62edbf",
   "metadata": {},
   "source": [
    "## Memory Optimization Note\n",
    "\n",
    "**Why Two Models?**\n",
    "- **Judge (Mistral 7B)**: Stays loaded to evaluate all comparisons\n",
    "- **Candidates (Qwen)**: Your Base/SFT/DPO models - loaded one at a time, then freed\n",
    "\n",
    "**Memory Usage:**\n",
    "- Mistral 7B (8-bit): ~12GB\n",
    "- Qwen 0.5B (fp16): ~1GB each\n",
    "- Total: Should fit in your 15GB T4\n",
    "\n",
    "**If you're running out of memory:**\n",
    "- Option 1: Pre-generate all responses, save to disk, then run judge (recommended)\n",
    "- Option 2: Use smaller judge model (Mistral-7B-Instruct-v0.1 or Llama-2-7B)\n",
    "- Option 3: Generate responses separately, then only load judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73f3ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured:\n",
      "  Base: Qwen/Qwen2.5-0.5B-Instruct\n",
      "  SFT: models/sft\n",
      "  DPO: models/dpo_enhanced\n"
     ]
    }
   ],
   "source": [
    "def load_candidate_model(model_path, is_adapter=False, base_model_name=\"Qwen/Qwen2.5-0.5B-Instruct\"):\n",
    "    \"\"\"Load a candidate model (base, SFT, or DPO).\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name if is_adapter else model_path)\n",
    "    \n",
    "    if is_adapter and os.path.exists(model_path):\n",
    "        # Load base model then adapter\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    else:\n",
    "        # Load regular model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Model configurations\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "SFT_MODEL_PATH = \"models/sft\"\n",
    "DPO_MODEL_PATH = \"models/dpo_enhanced\"\n",
    "\n",
    "# We'll load models on-demand to save memory\n",
    "print(\"Models configured:\")\n",
    "print(f\"  Base: {BASE_MODEL_NAME}\")\n",
    "print(f\"  SFT: {SFT_MODEL_PATH}\")\n",
    "print(f\"  DPO: {DPO_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dcfc6",
   "metadata": {},
   "source": [
    "## 4. Generate Responses from Candidate Models\n",
    "\n",
    "Get predictions from each model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4b8a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses from all models...\n",
      "\n",
      "Generating responses for Base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Base: 100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for SFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SFT: 100%|██████████| 50/50 [02:49<00:00,  3.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses from all models...\n",
      "\n",
      "Generating responses for Base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Base: 100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for SFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SFT: 100%|██████████| 50/50 [02:49<00:00,  3.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for DPO model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses from all models...\n",
      "\n",
      "Generating responses for Base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Base: 100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for SFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SFT: 100%|██████████| 50/50 [02:49<00:00,  3.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for DPO model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPO: 100%|██████████| 50/50 [03:02<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting responses from all models...\n",
      "\n",
      "Generating responses for Base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Base: 100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for SFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SFT: 100%|██████████| 50/50 [02:49<00:00,  3.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating responses for DPO model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPO: 100%|██████████| 50/50 [03:02<00:00,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Collected 150 responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>model</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269</td>\n",
       "      <td>Shout out James Boswell  #unibowl</td>\n",
       "      <td>1</td>\n",
       "      <td>Base</td>\n",
       "      <td>No. The given text does not contain any elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2255</td>\n",
       "      <td>My mouth while drunk….. unstoppable</td>\n",
       "      <td>0</td>\n",
       "      <td>Base</td>\n",
       "      <td>No. The text \"My mouth while drunk…. unstoppab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>444</td>\n",
       "      <td>\"Alexa add small bananas to the shopping list....</td>\n",
       "      <td>1</td>\n",
       "      <td>Base</td>\n",
       "      <td>No. The given text does not contain any sarcas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>219</td>\n",
       "      <td>Andrews really trying to explain to me that fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Base</td>\n",
       "      <td>Yes. The text uses sarcasm by suggesting that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3203</td>\n",
       "      <td>Investing doesn’t need to be complicated. Inve...</td>\n",
       "      <td>0</td>\n",
       "      <td>Base</td>\n",
       "      <td>Yes. The text uses sarcasm due to its ironic t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  true_label model  \\\n",
       "0    269                  Shout out James Boswell  #unibowl           1  Base   \n",
       "1   2255                My mouth while drunk….. unstoppable           0  Base   \n",
       "2    444  \"Alexa add small bananas to the shopping list....           1  Base   \n",
       "3    219  Andrews really trying to explain to me that fo...           1  Base   \n",
       "4   3203  Investing doesn’t need to be complicated. Inve...           0  Base   \n",
       "\n",
       "                                            response  \n",
       "0  No. The given text does not contain any elemen...  \n",
       "1  No. The text \"My mouth while drunk…. unstoppab...  \n",
       "2  No. The given text does not contain any sarcas...  \n",
       "3  Yes. The text uses sarcasm by suggesting that ...  \n",
       "4  Yes. The text uses sarcasm due to its ironic t...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, text, max_new_tokens=100):\n",
    "    \"\"\"Generate a response from a candidate model.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Is the following text sarcastic? Sarcasm often involves irony, exaggeration, or saying the opposite of what is meant. \n",
    "Answer with 'Yes' or 'No' and briefly explain your reasoning.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Answer:\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except:\n",
    "        prompt = messages[0]['content']\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def collect_all_responses(df_sample):\n",
    "    \"\"\"Collect responses from all three models.\"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    models_to_evaluate = [\n",
    "        (\"Base\", BASE_MODEL_NAME, False),\n",
    "        (\"SFT\", SFT_MODEL_PATH, True),\n",
    "        (\"DPO\", DPO_MODEL_PATH, True)\n",
    "    ]\n",
    "    \n",
    "    for model_name, model_path, is_adapter in models_to_evaluate:\n",
    "        print(f\"\\nGenerating responses for {model_name} model...\")\n",
    "        \n",
    "        if not os.path.exists(model_path) and is_adapter:\n",
    "            print(f\"  ⚠️ Model not found at {model_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        model, tokenizer = load_candidate_model(model_path, is_adapter, BASE_MODEL_NAME)\n",
    "        \n",
    "        for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=model_name):\n",
    "            response = generate_response(model, tokenizer, row['tweet'])\n",
    "            responses.append({\n",
    "                'index': idx,\n",
    "                'text': row['tweet'],\n",
    "                'true_label': row['sarcastic'],\n",
    "                'model': model_name,\n",
    "                'response': response\n",
    "            })\n",
    "        \n",
    "        # Free memory\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return pd.DataFrame(responses)\n",
    "\n",
    "# Generate all responses\n",
    "print(\"Collecting responses from all models...\")\n",
    "df_responses = collect_all_responses(df_sample)\n",
    "print(f\"\\n✓ Collected {len(df_responses)} responses\")\n",
    "df_responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f67c0",
   "metadata": {},
   "source": [
    "## 5. Judge Prompt Template\n",
    "\n",
    "Define how the judge will evaluate response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dad27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_judge_prompt(text, true_label, response_a, response_b, model_a_name, model_b_name):\n",
    "    \"\"\"Create a prompt for the judge to compare two responses.\"\"\"\n",
    "    label_text = \"sarcastic\" if true_label == 1 else \"not sarcastic\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert judge evaluating sarcasm detection model responses. Your task is to compare two model responses and determine which one better identifies sarcasm.\n",
    "\n",
    "**Text to analyze:** \"{text}\"\n",
    "\n",
    "**Ground Truth:** This text is {label_text}.\n",
    "\n",
    "**Model A ({model_a_name}) Response:**\n",
    "{response_a}\n",
    "\n",
    "**Model B ({model_b_name}) Response:**\n",
    "{response_b}\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. **Correctness**: Does the response match the ground truth?\n",
    "2. **Reasoning Quality**: Is the explanation clear, specific, and insightful?\n",
    "3. **Confidence**: Does the model show appropriate confidence?\n",
    "4. **Explanation Depth**: Does it identify sarcasm indicators (irony, exaggeration, context)?\n",
    "\n",
    "**Instructions:**\n",
    "- Compare both responses based on the criteria above\n",
    "- Choose the better response (A, B, or Tie if they're equal)\n",
    "- Explain your reasoning in 2-3 sentences\n",
    "\n",
    "**Your Judgment:**\n",
    "Winner: [A/B/Tie]\n",
    "Reasoning:\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da628809",
   "metadata": {},
   "source": [
    "## 6. Run Judge Evaluations\n",
    "\n",
    "Compare model pairs using the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c658e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_verdict(judge_client, judge_model_name, prompt, max_tokens=200):\n",
    "    \"\"\"Get judge's verdict on a comparison using Zephyr via API.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    try:\n",
    "        response = judge_client.chat_completion(\n",
    "            model=judge_model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.3,  # Lower temperature for more consistent judgments\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting judge verdict: {e}\")\n",
    "        return \"Error: Unable to get judgment\"\n",
    "\n",
    "def parse_judge_verdict(verdict_text):\n",
    "    \"\"\"Parse judge's verdict to extract winner.\"\"\"\n",
    "    verdict_lower = verdict_text.lower()\n",
    "    \n",
    "    # Look for explicit winner declaration\n",
    "    if 'winner: a' in verdict_lower or 'winner:a' in verdict_lower or 'winner a' in verdict_lower:\n",
    "        return 'A'\n",
    "    elif 'winner: b' in verdict_lower or 'winner:b' in verdict_lower or 'winner b' in verdict_lower:\n",
    "        return 'B'\n",
    "    elif 'winner: tie' in verdict_lower or 'winner:tie' in verdict_lower or 'tie' in verdict_lower[:50]:\n",
    "        return 'Tie'\n",
    "    \n",
    "    # Fallback: look for mentions in first 100 chars\n",
    "    first_part = verdict_lower[:100]\n",
    "    if 'model a' in first_part or 'response a' in first_part:\n",
    "        if 'model b' not in first_part and 'response b' not in first_part:\n",
    "            return 'A'\n",
    "    if 'model b' in first_part or 'response b' in first_part:\n",
    "        if 'model a' not in first_part and 'response a' not in first_part:\n",
    "            return 'B'\n",
    "    \n",
    "    # If unclear, return Tie\n",
    "    return 'Tie'\n",
    "\n",
    "def run_pairwise_comparison(df_responses, model_a_name, model_b_name):\n",
    "    \"\"\"Run pairwise comparison between two models using Zephyr judge.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Comparing {model_a_name} vs {model_b_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get responses for each model\n",
    "    df_a = df_responses[df_responses['model'] == model_a_name].set_index('index')\n",
    "    df_b = df_responses[df_responses['model'] == model_b_name].set_index('index')\n",
    "    \n",
    "    # Find common indices\n",
    "    common_indices = df_a.index.intersection(df_b.index)\n",
    "    \n",
    "    if len(common_indices) == 0:\n",
    "        print(f\"  ⚠️ No common samples found. Skipping comparison.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating {len(common_indices)} sample pairs...\")\n",
    "    \n",
    "    comparisons = []\n",
    "    \n",
    "    for idx in tqdm(common_indices, desc=\"Zephyr Judging\"):\n",
    "        row_a = df_a.loc[idx]\n",
    "        row_b = df_b.loc[idx]\n",
    "        \n",
    "        # Create judge prompt\n",
    "        prompt = create_judge_prompt(\n",
    "            row_a['text'],\n",
    "            row_a['true_label'],\n",
    "            row_a['response'],\n",
    "            row_b['response'],\n",
    "            model_a_name,\n",
    "            model_b_name\n",
    "        )\n",
    "        \n",
    "        # Get verdict from Zephyr\n",
    "        verdict = get_judge_verdict(judge_client, judge_model_name, prompt)\n",
    "        winner = parse_judge_verdict(verdict)\n",
    "        \n",
    "        comparisons.append({\n",
    "            'index': idx,\n",
    "            'text': row_a['text'],\n",
    "            'true_label': row_a['true_label'],\n",
    "            'model_a': model_a_name,\n",
    "            'model_b': model_b_name,\n",
    "            'response_a': row_a['response'],\n",
    "            'response_b': row_b['response'],\n",
    "            'winner': winner,\n",
    "            'judge_reasoning': verdict\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparisons)\n",
    "    \n",
    "    # Calculate win rates\n",
    "    win_counts = df_comparison['winner'].value_counts()\n",
    "    total = len(df_comparison)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Results: {model_a_name} vs {model_b_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  {model_a_name} wins: {win_counts.get('A', 0)} ({win_counts.get('A', 0)/total:.1%})\")\n",
    "    print(f\"  {model_b_name} wins: {win_counts.get('B', 0)} ({win_counts.get('B', 0)/total:.1%})\")\n",
    "    print(f\"  Ties: {win_counts.get('Tie', 0)} ({win_counts.get('Tie', 0)/total:.1%})\")\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "# Run all pairwise comparisons\n",
    "comparisons = {}\n",
    "\n",
    "# Base vs SFT\n",
    "if 'Base' in df_responses['model'].values and 'SFT' in df_responses['model'].values:\n",
    "    comparisons['Base_vs_SFT'] = run_pairwise_comparison(df_responses, 'Base', 'SFT')\n",
    "\n",
    "# SFT vs DPO\n",
    "if 'SFT' in df_responses['model'].values and 'DPO' in df_responses['model'].values:\n",
    "    comparisons['SFT_vs_DPO'] = run_pairwise_comparison(df_responses, 'SFT', 'DPO')\n",
    "\n",
    "# Base vs DPO\n",
    "if 'Base' in df_responses['model'].values and 'DPO' in df_responses['model'].values:\n",
    "    comparisons['Base_vs_DPO'] = run_pairwise_comparison(df_responses, 'Base', 'DPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e42fa",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Create visualizations comparing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_results(comparisons):\n",
    "    \"\"\"Create visualizations of comparison results.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(comparisons), figsize=(6*len(comparisons), 5))\n",
    "    \n",
    "    if len(comparisons) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (comparison_name, df_comp) in enumerate(comparisons.items()):\n",
    "        if df_comp is None:\n",
    "            continue\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Count wins\n",
    "        win_counts = df_comp['winner'].value_counts()\n",
    "        \n",
    "        # Create labels\n",
    "        model_a = df_comp['model_a'].iloc[0]\n",
    "        model_b = df_comp['model_b'].iloc[0]\n",
    "        \n",
    "        labels = [model_a, model_b, 'Tie']\n",
    "        values = [win_counts.get('A', 0), win_counts.get('B', 0), win_counts.get('Tie', 0)]\n",
    "        colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{int(height)}\\n({height/len(df_comp):.1%})',\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylabel('Number of Wins', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_a} vs {model_b}', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylim(0, max(values) * 1.2)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('llm_judge_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✓ Visualization saved as 'llm_judge_comparison.png'\")\n",
    "\n",
    "plot_comparison_results(comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee33b3",
   "metadata": {},
   "source": [
    "## 8. Analyze Judge Reasoning\n",
    "\n",
    "Extract insights from the judge's explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_judge_reasoning(comparisons):\n",
    "    \"\"\"Analyze patterns in judge reasoning.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUALITATIVE ANALYSIS: Judge Reasoning Patterns\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for comparison_name, df_comp in comparisons.items():\n",
    "        if df_comp is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n## {comparison_name.replace('_', ' ')}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        model_a = df_comp['model_a'].iloc[0]\n",
    "        model_b = df_comp['model_b'].iloc[0]\n",
    "        \n",
    "        # Show examples where Model A won\n",
    "        a_wins = df_comp[df_comp['winner'] == 'A'].head(2)\n",
    "        if len(a_wins) > 0:\n",
    "            print(f\"\\n### Example where {model_a} won:\")\n",
    "            for idx, row in a_wins.iterrows():\n",
    "                print(f\"\\n**Text:** {row['text'][:100]}...\")\n",
    "                print(f\"**Judge's reasoning:** {row['judge_reasoning'][:300]}...\")\n",
    "                break\n",
    "        \n",
    "        # Show examples where Model B won\n",
    "        b_wins = df_comp[df_comp['winner'] == 'B'].head(2)\n",
    "        if len(b_wins) > 0:\n",
    "            print(f\"\\n### Example where {model_b} won:\")\n",
    "            for idx, row in b_wins.iterrows():\n",
    "                print(f\"\\n**Text:** {row['text'][:100]}...\")\n",
    "                print(f\"**Judge's reasoning:** {row['judge_reasoning'][:300]}...\")\n",
    "                break\n",
    "\n",
    "analyze_judge_reasoning(comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245e2f2",
   "metadata": {},
   "source": [
    "## 9. Generate Comprehensive Report\n",
    "\n",
    "Create a detailed evaluation report for your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ec736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_research_report(comparisons, df_responses):\n",
    "    \"\"\"Generate a comprehensive research report.\"\"\"\n",
    "    report = {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'judge_model': 'HuggingFaceH4/zephyr-7b-beta',\n",
    "        'judge_method': 'LLM-as-a-Judge via HF Inference API',\n",
    "        'candidate_models': df_responses['model'].unique().tolist(),\n",
    "        'sample_size': len(df_responses) // len(df_responses['model'].unique()),\n",
    "        'comparisons': {}\n",
    "    }\n",
    "    \n",
    "    for comparison_name, df_comp in comparisons.items():\n",
    "        if df_comp is None:\n",
    "            continue\n",
    "        \n",
    "        win_counts = df_comp['winner'].value_counts()\n",
    "        total = len(df_comp)\n",
    "        \n",
    "        report['comparisons'][comparison_name] = {\n",
    "            'model_a': df_comp['model_a'].iloc[0],\n",
    "            'model_b': df_comp['model_b'].iloc[0],\n",
    "            'total_comparisons': total,\n",
    "            'model_a_wins': int(win_counts.get('A', 0)),\n",
    "            'model_b_wins': int(win_counts.get('B', 0)),\n",
    "            'ties': int(win_counts.get('Tie', 0)),\n",
    "            'model_a_win_rate': float(win_counts.get('A', 0) / total),\n",
    "            'model_b_win_rate': float(win_counts.get('B', 0) / total),\n",
    "            'tie_rate': float(win_counts.get('Tie', 0) / total)\n",
    "        }\n",
    "    \n",
    "    # Save report\n",
    "    with open('zephyr_judge_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESEARCH REPORT SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    print(\"\\n✓ Full report saved to 'zephyr_judge_report.json'\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = generate_research_report(comparisons, df_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af3381",
   "metadata": {},
   "source": [
    "## 10. Save Detailed Comparison Data\n",
    "\n",
    "Export all comparison data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all comparison dataframes\n",
    "for comparison_name, df_comp in comparisons.items():\n",
    "    if df_comp is not None:\n",
    "        filename = f'zephyr_judge_{comparison_name}.csv'\n",
    "        df_comp.to_csv(filename, index=False)\n",
    "        print(f\"✓ Saved {filename}\")\n",
    "\n",
    "# Save all responses\n",
    "df_responses.to_csv('zephyr_judge_all_responses.csv', index=False)\n",
    "print(f\"✓ Saved zephyr_judge_all_responses.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  1. llm_judge_comparison.png - Visualization\")\n",
    "print(\"  2. zephyr_judge_report.json - Structured results with reasoning\")\n",
    "print(\"  3. zephyr_judge_*_vs_*.csv - Detailed comparisons with judge explanations\")\n",
    "print(\"  4. zephyr_judge_all_responses.csv - All model responses\")\n",
    "print(\"\\n✓ Zephyr-based LLM-as-a-Judge evaluation completed!\")\n",
    "print(\"  Method: Preference-based evaluation with detailed reasoning\")\n",
    "print(\"  Judge: Zephyr-7B-beta via HF Inference API\")\n",
    "print(\"\\nUse these files for your research paper and analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045a716",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance Testing (Optional)\n",
    "\n",
    "Test if the differences between models are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def test_statistical_significance(df_comp):\n",
    "    \"\"\"Test if win rate differences are statistically significant.\"\"\"\n",
    "    win_counts = df_comp['winner'].value_counts()\n",
    "    \n",
    "    # Create contingency table\n",
    "    observed = [\n",
    "        [win_counts.get('A', 0), win_counts.get('B', 0)]\n",
    "    ]\n",
    "    \n",
    "    # Expected frequencies (equal distribution)\n",
    "    total = win_counts.get('A', 0) + win_counts.get('B', 0)\n",
    "    expected = [[total/2, total/2]]\n",
    "    \n",
    "    # Chi-square test\n",
    "    if total > 0:\n",
    "        chi2, p_value = chi2_contingency(observed + expected)[:2]\n",
    "        \n",
    "        model_a = df_comp['model_a'].iloc[0]\n",
    "        model_b = df_comp['model_b'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n{model_a} vs {model_b}:\")\n",
    "        print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(f\"  ✓ Difference is statistically significant (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  ✗ Difference is NOT statistically significant (p >= 0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for comparison_name, df_comp in comparisons.items():\n",
    "    if df_comp is not None:\n",
    "        test_statistical_significance(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b036579",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **LLM-as-a-Judge** provides qualitative insights that metrics alone can't capture\n",
    "2. **Preference-based evaluation** aligns with DPO training methodology\n",
    "3. **Comparative analysis** shows training progression effectiveness\n",
    "\n",
    "### For Your Research Paper:\n",
    "- Use the win rates as evidence of model improvement\n",
    "- Include judge reasoning examples to illustrate quality differences\n",
    "- Compare judge evaluation with traditional metrics (accuracy, F1)\n",
    "- Discuss alignment between DPO training and judge preferences\n",
    "\n",
    "### To Extend This Analysis:\n",
    "- Increase `SAMPLE_SIZE` for more robust results\n",
    "- Test with different judge models (GPT-4, Claude, etc.)\n",
    "- Analyze specific sarcasm types (irony, exaggeration, etc.)\n",
    "- Create ensemble judgments with multiple judges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
