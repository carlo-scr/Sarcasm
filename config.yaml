# Configuration file for SFT + DPO Training Pipeline
# MobileLLM-350M for Sarcasm Detection

# Model Configuration
model:
  base_model: "facebook/MobileLLM-350M"
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  use_nested_quant: true

# LoRA Configuration
lora:
  r: 16              # LoRA rank (try: 8, 16, 32, 64)
  lora_alpha: 16     # LoRA scaling factor
  lora_dropout: 0.1  # Dropout probability
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# SFT Training Configuration
sft:
  output_dir: "./results/sft"
  num_train_epochs: 5
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  learning_rate: 1e-4
  warmup_steps: 100
  logging_steps: 50
  eval_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  fp16: true
  max_grad_norm: 1.0
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: "none"

# DPO Training Configuration
dpo:
  output_dir: "./results/dpo"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 5e-5
  warmup_steps: 50
  logging_steps: 25
  eval_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 5
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  fp16: true
  max_grad_norm: 0.5
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  
  # DPO-specific hyperparameters
  beta: 0.1                    # KL penalty coefficient (CRITICAL: try 0.01, 0.02, 0.05, 0.1, 0.5)
  loss_type: "sigmoid"         # Options: sigmoid, hinge, ipo
  max_length: 512
  max_prompt_length: 256
  
  # Diagnostic thresholds
  kl_divergence_threshold: 1.0  # Warn if KL > this value
  reward_margin_threshold: 0.1  # Warn if chosen-rejected < this
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: "none"

# Dataset Configuration
data:
  train_file: "data/splits/gen_train.csv"
  test_file: "data/splits/gen_test.csv"
  text_column: "text"
  label_column: "class"
  sarcasm_label: "sarc"
  non_sarcasm_label: "notsarc"
  val_split: 0.15
  max_length: 512
  sample_size: null  # Set to int for subset training, null for full dataset

# Hyperparameter Search (for experimentation)
search:
  enabled: false
  sft_learning_rates: [5e-5, 1e-4, 2e-4]
  sft_batch_sizes: [8, 16, 32]
  sft_lora_ranks: [8, 16, 32, 64]
  dpo_betas: [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]
  dpo_learning_rates: [1e-5, 5e-5, 1e-4]
  dpo_loss_types: ["sigmoid", "hinge", "ipo"]

# Logging and Diagnostics
logging:
  log_dir: "./logs"
  save_diagnostics: true
  plot_training_curves: true
  log_model_outputs: true  # Log sample predictions during training
  num_sample_outputs: 10
  verbose: true
