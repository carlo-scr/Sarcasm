{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4cdda8",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a6630c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive to save models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create directory for saving models\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to save models\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory for saving models\n",
    "!mkdir -p /content/drive/MyDrive/sarcasm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (make sure repo is public first!)\n",
    "!git clone https://github.com/carlo-scr/Sarcasm.git\n",
    "%cd Sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65612490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets peft trl torch accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb99900",
   "metadata": {},
   "source": [
    "## 2. Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c31169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e66df9",
   "metadata": {},
   "source": [
    "## 3. Download SARC & Create Data Splits\n",
    "\n",
    "**Note:** SARC dataset (255MB) is downloaded directly from Princeton NLP to avoid uploading large files. iSarcasm is already in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SARC dataset from Kaggle (more reliable than Princeton NLP)\n",
    "print(\"Downloading SARC dataset from Kaggle...\")\n",
    "!pip install -q kagglehub\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Download from Kaggle\n",
    "path = kagglehub.dataset_download(\"danofer/sarcasm\")\n",
    "print(f\"✓ Downloaded to: {path}\")\n",
    "\n",
    "# Find the train-balanced-sarcasm.csv file\n",
    "!mkdir -p data/SARC\n",
    "csv_file = None\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if 'train-balanced-sarcasm' in file:\n",
    "            csv_file = os.path.join(root, file)\n",
    "            break\n",
    "    if csv_file:\n",
    "        break\n",
    "\n",
    "if csv_file:\n",
    "    shutil.copy(csv_file, 'data/SARC/train-balanced-sarcasm.csv')\n",
    "    print(f\"✓ SARC dataset ready ({os.path.getsize('data/SARC/train-balanced-sarcasm.csv') / 1e6:.1f} MB)\")\n",
    "else:\n",
    "    print(\"❌ Could not find train-balanced-sarcasm.csv in downloaded files\")\n",
    "    print(f\"Available files: {os.listdir(path)}\")\n",
    "\n",
    "# Create train/test splits for iSarcasm (already in repo)\n",
    "!python scripts/create_splits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e95fac",
   "metadata": {},
   "source": [
    "## 4. Phase 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Train on SARC dataset to learn general sarcasm patterns.\n",
    "\n",
    "**Time:** ~5-10 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/finetune_qwen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e4ed1",
   "metadata": {},
   "source": [
    "## 5. Phase 2: Direct Preference Optimization (DPO)\n",
    "\n",
    "Refine with iSarcasm preferences for better accuracy.\n",
    "\n",
    "**Time:** ~3-5 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ecc08",
   "metadata": {},
   "source": [
    "## 5A. Mine Hard Negatives (Optional but Recommended)\n",
    "\n",
    "Find SFT model's confident mistakes to create targeted training data for DPO.\n",
    "\n",
    "**Time:** ~2-3 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine hard negatives from SFT model's confident mistakes\n",
    "!python scripts/mine_hard_negatives.py\n",
    "\n",
    "# Show summary\n",
    "import json\n",
    "with open('data/hard_negatives.json', 'r') as f:\n",
    "    hard_negs = json.load(f)\n",
    "\n",
    "print(f\"\\n✓ Mined {len(hard_negs)} hard negatives\")\n",
    "print(f\"  False Positives: {sum(1 for hn in hard_negs if hn['predicted_label'] == 1)}\")\n",
    "print(f\"  False Negatives: {sum(1 for hn in hard_negs if hn['predicted_label'] == 0)}\")\n",
    "print(f\"\\nThese will be used to create targeted preference pairs in DPO training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144336b8",
   "metadata": {},
   "source": [
    "## 5B. Enhanced DPO Training\n",
    "\n",
    "Now train DPO with:\n",
    "- ✅ Explicit reasoning in preference pairs\n",
    "- ✅ Hard negatives from SFT mistakes\n",
    "- ✅ Confidence-weighted learning\n",
    "\n",
    "**Time:** ~3-5 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8af86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/dpo_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbb662",
   "metadata": {},
   "source": [
    "## 6. Evaluate All Models\n",
    "\n",
    "Compare Base → SFT → DPO on held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/evaluate_all_stages.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967975c",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('comparative_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model in results['models']:\n",
    "    print(f\"\\n{model['model_name']}:\")\n",
    "    print(f\"  Accuracy:  {model['accuracy']:.2%}\")\n",
    "    print(f\"  Precision: {model['precision']:.2%}\")\n",
    "    print(f\"  Recall:    {model['recall']:.2%}\")\n",
    "    print(f\"  F1 Score:  {model['f1_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e0165",
   "metadata": {},
   "source": [
    "## 8. Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "!cp -r models/sft /content/drive/MyDrive/sarcasm_models/\n",
    "!cp -r models/dpo_enhanced /content/drive/MyDrive/sarcasm_models/\n",
    "!cp comparative_results.json /content/drive/MyDrive/sarcasm_models/\n",
    "\n",
    "print(\"✓ Models saved to Google Drive: MyDrive/sarcasm_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adba39c",
   "metadata": {},
   "source": [
    "## 9. Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download results JSON\n",
    "files.download('comparative_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d1440",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training Complete!**\n",
    "\n",
    "Models saved to:\n",
    "- Google Drive: `MyDrive/sarcasm_models/`\n",
    "- Local (Colab): `models/sft/` and `models/dpo_enhanced/`\n",
    "\n",
    "**Expected Performance:**\n",
    "- Base Model: ~49% accuracy\n",
    "- SFT Model: ~63% accuracy (+14 pts)\n",
    "- DPO Model: ~68% accuracy (+5 pts)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download models from Google Drive\n",
    "2. Use for inference in your application\n",
    "3. Upload to Hugging Face Hub for sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
